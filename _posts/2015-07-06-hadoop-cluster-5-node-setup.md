---
layout: post
title: "hadoop cluster 5 node setup"
description: ""
category: [hadoop]
tags: [ubuntu, ssh, static ip]
---
{% include JB/setup %}


### steps

1. install ubuntu 15.04 desktop on 5 nodes

1. setup ssh

1. setup hadoop

1. config and start

1. fixed

### 1. setup ubuntu

1. download and install ubuntu desktop on 5 nodes

1. setup static ip address for 5 nodes

    1. node5

            $ sudo pico /etc/network/interfaces
            # interfaces(5) file used by ifup(8) and ifdown(8)
            auto lo
            iface lo inet loopback

            auto eth0
            iface eth0 inet static
            address 192.168.120.155
            netmask 255.255.255.0
            network 192.168.120.0
            broadcast 192.168.120.255
            gateway 192.168.120.1
            dns-nameservers 192.168.10.220 192.168.10.221

            $ sudo pico /etc/resolv.conf 
            # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
            #     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN

            nameserver 192.168.10.220
            nameserver 192.168.10.221

            $ sudo pico /etc/hosts
            127.0.0.1   localhost
            127.0.1.1   node5

            # The following lines are desirable for IPv6 capable hosts
            ::1     ip6-localhost ip6-loopback
            fe00::0 ip6-localnet
            ff00::0 ip6-mcastprefix
            ff02::1 ip6-allnodes
            ff02::2 ip6-allrouters

            192.168.120.155 node5
            192.168.120.154 node4
            192.168.120.153 node3
            192.168.120.152 node2
            192.168.120.151 node1

            $ ping www.baidu.com

    1. node1 node2 node3 node4

            # same as node5

    1. mac os x yosemite

            $ pico /private/etc/hosts
            ##
            # Host Database
            #
            # localhost is used to configure the loopback interface
            # when the system is booting.  Do not change this entry.
            ##
            127.0.0.1   localhost
            255.255.255.255 broadcasthost
            ::1             localhost 

            192.168.120.155 node5
            192.168.120.154 node4
            192.168.120.153 node3
            192.168.120.152 node2
            192.168.120.151 node1

1. change apt sources

            $ sudo sed 's@cn.archive.ubuntu.com@mirrors.163.com@' -i /etc/apt/sources.list
            $ sudo sed 's@security.ubuntu.com@mirrors.163.com@' -i /etc/apt/sources.list

1. set HADOOP_HOME JAVA_HOME

            $ sudo pico .bashrc

            ...
            export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre
            export HADOOP_HOME=~/node5/hadoop
            export PATH=$PATH:$HADOOP_HOME
            export PATH=$PATH:$JAVA_HOME
            ...

            $ source .bashrc

1. firewall

    1. check status of firewall

            $ sudo ufw status

    1. stop iptables service

            $ sudo ufw disable

    1. reload / restart iptables service

            $ sudo ufw reload

    1. get ipv4 iptables status

            $ sudo iptables -L -n -v

    1. get ipv6 ip6tables status

            $ sudo ip6tables -L -n -v

1. add user

            $ sudo addgroup hadoop
            $ sudo adduser --ingroup hadoop hduser
            $ sudo chown -R hduser:hadoop /opt/bigdata

### 2. setup ssh

1. install [openssh-server](http://www.openssh.com/)

    1. node1 to node5

            $ sudo apt-get install openssh-server

1. generate ssh key

    1. node5
            $ su - hduser
            $ ssh-keygen -t rsa -P "" 
            $ cd ~/.ssh
            $ cat id_rsa.pub >> authorized_keys

1. setup passphraseless ssh

            $ ssh-copy-id -i id_rsa.pub hduser@node1
            $ ssh-copy-id -i id_rsa.pub hduser@node2
            $ ssh-copy-id -i id_rsa.pub hduser@node3
            $ ssh-copy-id -i id_rsa.pub hduser@node4

1. check ssh login

            $ ssh hduser@node1
            ...
            exit

            $ ssh hduser@node2
            ...
            exit

            $ ssh hduser@node3
            ...
            exit

            $ ssh hduser@node4
            ...
            exit

1. [remove key from known hosts](http://superuser.com/questions/30087/remove-key-from-known-hosts)

            $ rm -f .ssh/known_hosts

            or

            $ ssh-keygen -R "hduser"

### 3. setup hadoop

1. cp software to node

            $ scp hadoop-2.7.0.tar.gz node5@node5:~/

1. install hadoop on node5 to node1

    1. mkdir

            $ ssh hduser@node5
            $ sudo mkdir /opt/bigdata
            $ sudo chown -R hduser:hadoop /opt/bigdata
            $ exit

            ...
            # node4 node3 node2
            ...

            $ ssh hduser@node1
            $ sudo mkdir /opt/bigdata
            $ sudo chown -R hduser:node1 /opt/bigdata
            $ exit

    1. unzip and config [scp](http://www.joycebabu.com/blog/copying-multiple-files-with-scp.html)

            $ tar -zxvf hadoop-2.7.0.tar.gz
            $ mv hadoop-2.7.0 /opt/bigdata/hadoop

            $ scp core-site.xml hdfs-site.xml mapred-site.xml slaves yarn-site.xml \
            hduser@node5:/opt/bigdata/hadoop/etc/hadoop

            $ pico .bashrc
            ...
            export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre
            export HADOOP_PREFIX="/opt/bigdata/hadoop"
            export HADOOP_HOME=$HADOOP_PREFIX
            export HADOOP_COMMON_HOME=$HADOOP_PREFIX
            export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop
            export HADOOP_HDFS_HOME=$HADOOP_PREFIX
            export HADOOP_MAPRED_HOME=$HADOOP_PREFIX
            export HADOOP_YARN_HOME=$HADOOP_PREFIX
            export PATH=$PATH:$HADOOP_HOME
            export PATH=$PATH:$JAVA_HOME
            ...

    1. sync folder

            $ scp .bashrc hduser@node4:~/
            $ scp .bashrc hduser@node3:~/
            $ scp .bashrc hduser@node2:~/
            $ scp .bashrc hduser@node1:~/

            $ scp -r /opt/bigdata/hadoop hduser@node4:/opt/bigdata/hadoop
            $ scp -r /opt/bigdata/hadoop hduser@node3:/opt/bigdata/hadoop
            $ scp -r /opt/bigdata/hadoop hduser@node2:/opt/bigdata/hadoop
            $ scp -r /opt/bigdata/hadoop hduser@node1:/opt/bigdata/hadoop
            $ scp -r /opt/bigdata/hadoop/etc/hadoop hduser@node4:/opt/bigdata/hadoop/etc
            $ scp -r /opt/bigdata/hadoop/etc/hadoop hduser@node3:/opt/bigdata/hadoop/etc
            $ scp -r /opt/bigdata/hadoop/etc/hadoop hduser@node2:/opt/bigdata/hadoop/etc
            $ scp -r /opt/bigdata/hadoop/etc/hadoop hduser@node1:/opt/bigdata/hadoop/etc

### config and start

1. config

    1. core-site.xml

            <?xml version="1.0" encoding="UTF-8"?>
            <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
                <property>
                    <name>fs.default.name</name>
                    <value>hdfs://node5:9000</value>
                </property>
                <property>
                    <name>hadoop.tmp.dir</name>
                    <value>file:///app/hadoop/tmp</value>
                </property>
            </configuration>

    1. hdfs-site.xml

            <?xml version="1.0" encoding="UTF-8"?>
            <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
                <property>
                    <name>dfs.namenode.checkpoint.period</name>
                    <value>3600</value>
                </property>
                <property>
                    <name>dfs.namenode.name.dir</name>
                    <value>file:///app/hadoop/hdfs/name</value>
                    <final>true</final>
                </property>
                <property>
                    <name>dfs.datanode.data.dir</name>
                    <value>file:///app/hadoop/hdfs/data</value>
                    <final>true</final>
                </property>
                <property>
                    <name>dfs.blocksize</name>
                    <value>134217728</value>
                </property>
                <property>
                    <name>dfs.replication</name>
                    <value>3</value>
                </property>
                <property>
                    <name>dfs.permissions</name>
                    <value>false</value>
                </property>
                <property>
                    <name>dfs.namenode.handler.count</name>
                    <value>50</value>
                </property>
                <property>
                    <name>dfs.namenode.checkpoint.dir</name>
                    <value>file:///app/hadoop/hdfs/namesecondary</value>
                </property>
            </configuration>

    1. yarn-site.xml

            <?xml version="1.0"?>
            <configuration>
                <property>
                    <name>yarn.nodemanager.aux-services</name>
                    <value>mapreduce_shuffle</value>
                </property>
                <property>
                    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
                    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
                </property>
                <property>
                    <name>yarn.resourcemanager.resource-tracker.address</name>
                    <value>node5:8025</value>
                </property>
                <property>
                    <name>yarn.resourcemanager.scheduler.address</name>
                    <value>node5:8030</value>
                </property>
                <property>
                    <name>yarn.resourcemanager.address</name>
                    <value>node5:8040</value>
                </property>
            </configuration>

    1. mapred-site.xml

            <?xml version="1.0"?>
            <configuration>
                <property>
                    <name>mapreduce.framework.name</name>
                    <value>yarn</value>
                </property>
            </configuration>

    1. slaves

            node4
            node3
            node2
            node1

1. format and start

    1. on node5 format hdfs

            $ bin/hdfs namenode -format

            # exit safemode
            $ bin/hdfs dfsadmin -report
            Configured Capacity: 5854310649856 (5.32 TB)
            Present Capacity: 5532915458048 (5.03 TB)
            DFS Remaining: 5532915384320 (5.03 TB)
            DFS Used: 73728 (72 KB)
            DFS Used%: 0.00%
            Under replicated blocks: 0
            Blocks with corrupt replicas: 0
            Missing blocks: 0
            Missing blocks (with replication factor 1): 0

            -------------------------------------------------
            Live datanodes (3):

            Name: 192.168.120.154:50010 (node4)
            Hostname: node4
            Decommission Status : Normal
            Configured Capacity: 1951699709952 (1.78 TB)
            DFS Used: 24576 (24 KB)
            Non DFS Used: 103953088512 (96.81 GB)
            DFS Remaining: 1847746596864 (1.68 TB)
            DFS Used%: 0.00%
            DFS Remaining%: 94.67%
            Configured Cache Capacity: 0 (0 B)
            Cache Used: 0 (0 B)
            Cache Remaining: 0 (0 B)
            Cache Used%: 100.00%
            Cache Remaining%: 0.00%
            Xceivers: 1
            Last contact: Wed Jul 08 08:50:45 CST 2015


            Name: 192.168.120.152:50010 (node2)
            Hostname: node2
            Decommission Status : Normal
            Configured Capacity: 1951307567104 (1.77 TB)
            DFS Used: 24576 (24 KB)
            Non DFS Used: 104823349248 (97.62 GB)
            DFS Remaining: 1846484193280 (1.68 TB)
            DFS Used%: 0.00%
            DFS Remaining%: 94.63%
            Configured Cache Capacity: 0 (0 B)
            Cache Used: 0 (0 B)
            Cache Remaining: 0 (0 B)
            Cache Used%: 100.00%
            Cache Remaining%: 0.00%
            Xceivers: 1
            Last contact: Wed Jul 08 08:50:45 CST 2015


            Name: 192.168.120.153:50010 (node3)
            Hostname: node3
            Decommission Status : Normal
            Configured Capacity: 1951303372800 (1.77 TB)
            DFS Used: 24576 (24 KB)
            Non DFS Used: 112618754048 (104.88 GB)
            DFS Remaining: 1838684594176 (1.67 TB)
            DFS Used%: 0.00%
            DFS Remaining%: 94.23%
            Configured Cache Capacity: 0 (0 B)
            Cache Used: 0 (0 B)
            Cache Remaining: 0 (0 B)
            Cache Used%: 100.00%
            Cache Remaining%: 0.00%
            Xceivers: 1
            Last contact: Wed Jul 08 08:50:45 CST 2015

    1. on node5 start

            $ sbin/start-dfs.sh
            $ sbin/start-yarn.sh

    1. on node5 stop

            $ sbin/stop-yarn.sh
            $ sbin/stop-dfs.sh

### fixed

1. problem connecting to server

    1. log on node3

            2015-07-08 08:20:39,044 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: node5/192.168.120.155:9000

            2015-07-08 08:20:48,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: node5/192.168.120.155:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)

    1. [datanode can't talk to namenode](http://stackoverflow.com/questions/30215825/datanodes-cant-talk-to-namenode)

    1. [hdfs service failed to start, issues with datanode and namenode](https://groups.google.com/a/cloudera.org/forum/#!topic/cdh-user/pWAjcfBnueo)

            Harsh J

            Sorry, should've sent you a direct stringâ€¦ I mean something like this: 

            127.0.0.1 localhost johniv-able 
            # 127.0.1.1 johniv-able 
            # ^^^^^^^^^^^^^^^^^^^^^
