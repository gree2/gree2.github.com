---
layout: post
title: "hadoop cluster 5 node setup"
description: ""
category: [hadoop]
tags: [ubuntu, ssh, static ip]
---
{% include JB/setup %}


### steps

1. install ubuntu 15.04 desktop on 5 nodes

1. setup ssh

1. setup hadoop

1. config and start

### 1. setup ubuntu

1. download and install ubuntu desktop on 5 nodes

1. setup static ip address for 5 nodes

    1. node5

            $ sudo pico /etc/network/interfaces
            # interfaces(5) file used by ifup(8) and ifdown(8)
            auto lo
            iface lo inet loopback

            auto eth0
            iface eth0 inet static
            address 192.168.120.155
            netmask 255.255.255.0
            network 192.168.120.0
            broadcast 192.168.120.255
            gateway 192.168.120.1
            dns-nameservers 192.168.10.220 192.168.10.221

            $ sudo pico /etc/resolv.conf 
            # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
            #     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN

            nameserver 192.168.10.220
            nameserver 192.168.10.221

            $ sudo pico /etc/hosts
            127.0.0.1   localhost
            127.0.1.1   node5

            # The following lines are desirable for IPv6 capable hosts
            ::1     ip6-localhost ip6-loopback
            fe00::0 ip6-localnet
            ff00::0 ip6-mcastprefix
            ff02::1 ip6-allnodes
            ff02::2 ip6-allrouters

            192.168.120.155 node5
            192.168.120.154 node4
            192.168.120.153 node3
            192.168.120.152 node2
            192.168.120.151 node1

            $ ping www.baidu.com

    1. node1 node2 node3 node4

            # same as node5

    1. mac os x yosemite

            $ pico /private/etc/hosts
            ##
            # Host Database
            #
            # localhost is used to configure the loopback interface
            # when the system is booting.  Do not change this entry.
            ##
            127.0.0.1   localhost
            255.255.255.255 broadcasthost
            ::1             localhost 

            192.168.120.155 node5
            192.168.120.154 node4
            192.168.120.153 node3
            192.168.120.152 node2
            192.168.120.151 node1

1. change apt sources

            $ sudo sed 's@cn.archive.ubuntu.com@mirrors.163.com@' -i /etc/apt/sources.list
            $ sudo sed 's@security.ubuntu.com@mirrors.163.com@' -i /etc/apt/sources.list

1. set HADOOP_HOME JAVA_HOME

            $ sudo pico .bashrc

            ...
            export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre
            export HADOOP_HOME=~/node5/hadoop
            export PATH=$PATH:$HADOOP_HOME
            export PATH=$PATH:$JAVA_HOME
            ...

            $ source .bashrc

1. firewall

    1. check status of firewall

            $ sudo ufw status

    1. stop iptables service

            $ sudo ufw disable

    1. reload / restart iptables service

            $ sudo ufw reload

    1. get ipv4 iptables status

            $ sudo iptables -L -n -v

    1. get ipv6 ip6tables status

            $ sudo ip6tables -L -n -v

1. add user

            $ sudo addgroup hadoop
            $ sudo adduser --ingroup hadoop hduser
            $ sudo chown -R hduser:hadoop /opt/bigdata

### 2. setup ssh

1. install [openssh-server](http://www.openssh.com/)

    1. node1 to node5

            $ sudo apt-get install openssh-server

1. generate ssh key

    1. node5
            $ su - hduser
            $ ssh-keygen -t rsa -P "" 
            $ cd ~/.ssh
            $ cat id_rsa.pub >> authorized_keys

1. setup passphraseless ssh

            $ ssh-copy-id -i id_rsa.pub hduser@node1
            $ ssh-copy-id -i id_rsa.pub hduser@node2
            $ ssh-copy-id -i id_rsa.pub hduser@node3
            $ ssh-copy-id -i id_rsa.pub hduser@node4

1. check ssh login

            $ ssh hduser@node1
            ...
            exit

            $ ssh hduser@node2
            ...
            exit

            $ ssh hduser@node3
            ...
            exit

            $ ssh hduser@node4
            ...
            exit

1. [remove key from known hosts](http://superuser.com/questions/30087/remove-key-from-known-hosts)

            $ rm -f .ssh/known_hosts

            or

            $ ssh-keygen -R "hduser"

### 3. setup hadoop

1. cp software to node

            $ scp hadoop-2.7.0.tar.gz node5@node5:~/

1. install hadoop on node5 to node1

    1. mkdir

            $ ssh hduser@node5
            $ sudo mkdir /opt/bigdata
            $ sudo chown -R hduser:hadoop /opt/bigdata
            $ exit

            ...
            # node4 node3 node2
            ...

            $ ssh hduser@node1
            $ sudo mkdir /opt/bigdata
            $ sudo chown -R hduser:node1 /opt/bigdata
            $ exit

    1. unzip and config [scp](http://www.joycebabu.com/blog/copying-multiple-files-with-scp.html)

            $ tar -zxvf hadoop-2.7.0.tar.gz
            $ mv hadoop-2.7.0 /opt/bigdata/hadoop

            $ scp core-site.xml hdfs-site.xml mapred-site.xml slaves yarn-site.xml \
            hduser@node5:/opt/bigdata/hadoop/etc/hadoop

            $ pico .bashrc
            ...
            export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre
            export HADOOP_PREFIX="/opt/bigdata/hadoop"
            export HADOOP_HOME=$HADOOP_PREFIX
            export HADOOP_COMMON_HOME=$HADOOP_PREFIX
            export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop
            export HADOOP_HDFS_HOME=$HADOOP_PREFIX
            export HADOOP_MAPRED_HOME=$HADOOP_PREFIX
            export HADOOP_YARN_HOME=$HADOOP_PREFIX
            export PATH=$PATH:$HADOOP_HOME
            export PATH=$PATH:$JAVA_HOME
            ...

    1. sync folder

            $ scp .bashrc hduser@node4:~/
            $ scp .bashrc hduser@node3:~/
            $ scp .bashrc hduser@node2:~/
            $ scp .bashrc hduser@node1:~/

            $ scp -r /opt/bigdata/hadoop hduser@node4:/opt/bigdata/hadoop
            $ scp -r /opt/bigdata/hadoop hduser@node3:/opt/bigdata/hadoop
            $ scp -r /opt/bigdata/hadoop hduser@node2:/opt/bigdata/hadoop
            $ scp -r /opt/bigdata/hadoop hduser@node1:/opt/bigdata/hadoop
            $ scp -r /opt/bigdata/hadoop/etc/hadoop hduser@node4:/opt/bigdata/hadoop/etc
            $ scp -r /opt/bigdata/hadoop/etc/hadoop hduser@node3:/opt/bigdata/hadoop/etc
            $ scp -r /opt/bigdata/hadoop/etc/hadoop hduser@node2:/opt/bigdata/hadoop/etc
            $ scp -r /opt/bigdata/hadoop/etc/hadoop hduser@node1:/opt/bigdata/hadoop/etc

### config and start

1. config

    1. core-site.xml

            <?xml version="1.0" encoding="UTF-8"?>
            <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
                <property>
                    <name>fs.default.name</name>
                    <value>hdfs://node5:9000</value>
                </property>
                <property>
                    <name>hadoop.tmp.dir</name>
                    <value>file:///app/hadoop/tmp</value>
                </property>
            </configuration>

    1. hdfs-site.xml

            <?xml version="1.0" encoding="UTF-8"?>
            <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
                <property>
                    <name>dfs.namenode.checkpoint.period</name>
                    <value>3600</value>
                </property>
                <property>
                    <name>dfs.namenode.name.dir</name>
                    <value>file:///app/hadoop/hdfs/name</value>
                    <final>true</final>
                </property>
                <property>
                    <name>dfs.datanode.data.dir</name>
                    <value>file:///app/hadoop/hdfs/data</value>
                    <final>true</final>
                </property>
                <property>
                    <name>dfs.blocksize</name>
                    <value>134217728</value>
                </property>
                <property>
                    <name>dfs.replication</name>
                    <value>3</value>
                </property>
                <property>
                    <name>dfs.permissions</name>
                    <value>false</value>
                </property>
                <property>
                    <name>dfs.namenode.handler.count</name>
                    <value>50</value>
                </property>
                <property>
                    <name>dfs.namenode.checkpoint.dir</name>
                    <value>file:///app/hadoop/hdfs/namesecondary</value>
                </property>
            </configuration>

    1. yarn-site.xml

            <?xml version="1.0"?>
            <configuration>
                <property>
                    <name>yarn.nodemanager.aux-services</name>
                    <value>mapreduce_shuffle</value>
                </property>
                <property>
                    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
                    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
                </property>
                <property>
                    <name>yarn.resourcemanager.resource-tracker.address</name>
                    <value>node5:8025</value>
                </property>
                <property>
                    <name>yarn.resourcemanager.scheduler.address</name>
                    <value>node5:8030</value>
                </property>
                <property>
                    <name>yarn.resourcemanager.address</name>
                    <value>node5:8040</value>
                </property>
            </configuration>

    1. mapred-site.xml

            <?xml version="1.0"?>
            <configuration>
                <property>
                    <name>mapreduce.framework.name</name>
                    <value>yarn</value>
                </property>
            </configuration>

    1. slaves

            node4
            node3
            node2
            node1

1. format and start

    1. format

            $ ./hdfs namenode -format

            # exit safemode
            $ ./hdfs dfsadmin -safemode leave

            # report
            $ ./hdfs dfsadmin -report

    1. start

            $ start-dfs.sh
            $ start-yarn.sh

    1. stop

            $ stop-yarn.sh
            $ stop-dfs.sh
